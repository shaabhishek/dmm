{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import kl_divergence\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_data(time_step = 10, num_sample = 16, marker_dim=3):\n",
    "    marker = torch.randn(time_step, num_sample, marker_dim).to(device)\n",
    "#     points_ = np.random.rand(time_step, num_sample) * 1.\n",
    "#     cum_sum_points =  np.cumsum(points_, axis = 0)\n",
    "#     t = np.stack([cum_sum_points, points_], axis = 2)\n",
    "#     x, t  = marker.tolist(), t.tolist()\n",
    "#     x = torch.tensor(x)\n",
    "#     t = torch.tensor(t)\n",
    "#     data = {'x':x, 't': t}\n",
    "    data = marker\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMM(nn.Module):\n",
    "    def __init__(self, marker_type='real', marker_dim=3, latent_dim=20):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus\n",
    "        \n",
    "        self.marker_type = marker_type\n",
    "        self.latent_dim = latent_dim\n",
    "        self.inference_hidden_dim = latent_dim\n",
    "        \n",
    "        self._inference_net(x_dim=marker_dim, z_dim=latent_dim, inference_hidden_dim=latent_dim)\n",
    "        self._gated_transition_net(latent_dim, latent_dim)\n",
    "        self._emission_net(z_dim=latent_dim, x_dim=marker_dim, emission_hidden_dim=latent_dim)\n",
    "\n",
    "    def _gated_transition_net(self, z_dim, transition_dim):\n",
    "        # mean\n",
    "        self.gating_unit_hidden = nn.Sequential(\n",
    "            nn.Linear(z_dim, transition_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.gating_unit_for_proposed_mean = nn.Sequential(\n",
    "            nn.Linear(transition_dim, z_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.nonlinear_proposed_mean = nn.Linear(transition_dim, z_dim)\n",
    "        \n",
    "        self.linear_mean = nn.Linear(z_dim, z_dim)\n",
    "        \n",
    "        self.linear_mean.weight.data = torch.eye(z_dim)\n",
    "        self.linear_mean.bias.data = torch.zeros(z_dim)\n",
    "        \n",
    "        # variance\n",
    "        self.z_next_var_net = nn.Sequential(\n",
    "            nn.Linear(z_dim, z_dim),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "    def _transition(self, z_prev):\n",
    "        \"\"\"\n",
    "            This is the function that computes the p_theta (z_t | z_{t-1})\n",
    "            Input:\n",
    "            z_prev: Tensor of shape TxBSxlatent_dim (if real)\n",
    "        \"\"\"\n",
    "        # Linear mean (linear function of previous state)\n",
    "        linear_proposed_mean = self.linear_mean(z_prev)\n",
    "        \n",
    "        # Nonlinear (proposed) mean using gates\n",
    "        hidden = self.gating_unit_hidden(z_prev)\n",
    "        gating_unit = self.gating_unit_for_proposed_mean(hidden)\n",
    "        proposed_mean = self.nonlinear_proposed_mean(hidden)\n",
    "        \n",
    "        # Combine nonlinear and linear options of means to choose\n",
    "        z_next_mean = (1 - gating_unit) * linear_proposed_mean \\\n",
    "                        + gating_unit*proposed_mean\n",
    "            \n",
    "        z_next_var_int = self.relu(proposed_mean)\n",
    "        z_next_var = self.z_next_var_net(z_next_var_int)\n",
    "        \n",
    "#         z_next_dist = Normal(z_next_mean, z_next_var.sqrt())\n",
    "#         z_next = z_next_dist.sample()\n",
    "        return z_next_mean, z_next_var\n",
    "\n",
    "    \n",
    "    def _emission_net(self, z_dim, x_dim, emission_hidden_dim):\n",
    "        self.emission_net = nn.Sequential(\n",
    "            nn.Linear(z_dim, emission_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emission_hidden_dim, emission_hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if self.marker_type == 'real':\n",
    "            self.x_mean = nn.Linear(emission_hidden_dim, x_dim)\n",
    "            self.x_var = nn.Sequential(\n",
    "                nn.Linear(emission_hidden_dim, x_dim),\n",
    "                nn.Softplus()\n",
    "            )\n",
    "        elif self.marker_type == 'binary':\n",
    "            pass\n",
    "    \n",
    "    def _emission(self, z_seq):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "            z_t: Tensor of shape TxBSxlatent_dim (if real)\n",
    "        \"\"\"\n",
    "        if self.marker_type == 'real':\n",
    "            x_mean = self.x_mean(z_seq)\n",
    "            x_var = self.x_var(z_seq)\n",
    "            x_dist = Normal(x_mean, x_var.sqrt())    \n",
    "        elif self.marker_type == 'binary':\n",
    "            pass\n",
    "#         x_sample = x_dist.sample()\n",
    "        return x_dist\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _combiner_fn(self, z_prev, rnn_state_right):\n",
    "        hidden_combined = (self.combiner_net(z_prev) + rnn_state_right)/3\n",
    "        \n",
    "        return hidden_combined\n",
    "    \n",
    "    def _inference_net(self, x_dim, z_dim, inference_hidden_dim):\n",
    "        \n",
    "        self.backward_rnn = nn.GRUCell(x_dim, inference_hidden_dim)\n",
    "        self.combiner_net = nn.Sequential(\n",
    "            nn.Linear(z_dim, inference_hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.posterior_mean_net = nn.Linear(inference_hidden_dim, z_dim)\n",
    "        self.posterior_var_net = nn.Sequential(\n",
    "            nn.Linear(inference_hidden_dim, z_dim),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "    def _inference(self, x_seq):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "            x_seq: Tensor of shape TxBSxmarker_dim (if real)\n",
    "        \"\"\"\n",
    "        # Generate z_0. Can choose to learn it as a parameter or fix it\n",
    "        # Recurrently, generate h_t\n",
    "        # Recurrently, generate z_t using z_{t-1} and h_t\n",
    "        T, BS, _ = x_seq.shape\n",
    "        z_dim = self.latent_dim\n",
    "        z_0 = nn.Parameter(torch.zeros(BS, z_dim)).to(device)\n",
    "        h_t = torch.zeros(BS, self.inference_hidden_dim).to(device)\n",
    "        z_seq = [z_0]\n",
    "        z_means = []\n",
    "        z_vars = []\n",
    "        \n",
    "        z_prev = z_0\n",
    "        # x_t[t] will be BS x marker_dim\n",
    "        # go backward, starting from T --> 1\n",
    "        for t in range(T-1,-1,-1):\n",
    "            h_t_right = self.backward_rnn(x_seq[t], h_t)\n",
    "            h_combined = self._combiner_fn(z_prev, h_t_right)\n",
    "            posterior_z_t_mean = self.posterior_mean_net(h_combined)\n",
    "            posterior_z_t_var = self.posterior_var_net(h_combined)\n",
    "            epsilon = torch.randn_like(posterior_z_t_mean)\n",
    "            posterior_z_t_sample = posterior_z_t_mean + posterior_z_t_var.sqrt()*epsilon\n",
    "\n",
    "            z_means.append(posterior_z_t_mean)\n",
    "            z_vars.append(posterior_z_t_var)\n",
    "            z_seq.append(posterior_z_t_sample)\n",
    "            z_prev = posterior_z_t_sample\n",
    "        \n",
    "        z_seq = torch.stack(z_seq, dim=0)\n",
    "        z_means = torch.stack(z_means, dim=0)\n",
    "        z_vars = torch.stack(z_vars, dim=0)\n",
    "        return z_seq, z_means, z_vars\n",
    "    \n",
    "    def forward(self, x_seq):\n",
    "        # Get a sampled sequence (z_0 --> z_T) and\n",
    "        # the means and the vars for\n",
    "        # q_phi(z_seq | x_seq) = q_phi(z_t | z_{t-1}, x_seq)\n",
    "        posterior_seq_sample, posterior_means, posterior_vars = self._inference(x_seq)\n",
    "        \n",
    "        # Get means and vars for p_theta (z) = p_theta (z_t | z_{t-1})\n",
    "        prior_means, prior_vars = self._transition(posterior_seq_sample[:-1])\n",
    "        \n",
    "        # Get p_theta (z_t | z_{t-1})\n",
    "        prior_dist = Normal(prior_means, prior_vars.sqrt())\n",
    "        \n",
    "        # Convert means and vars to torch distributions\n",
    "        posterior_dist = Normal(posterior_means, posterior_vars.sqrt())\n",
    "        \n",
    "        # Get p_theta(x_t | z_t)\n",
    "        reconstructed_x_dist = self._emission(posterior_seq_sample[:-1])\n",
    "        \n",
    "        # log p_theta (x_seq | z) = sum( log p_theta (x_t | z_t))\n",
    "        neg_log_likelihood = -reconstructed_x_dist.log_prob(x_seq).sum()\n",
    "        \n",
    "        # KL divergence between the variational posterior distribution and generative prior\n",
    "        # KL (q_phi (z_t | z_{t-1}, x_seq) || p_theta (z_t | z_{t-1}))\n",
    "        kl = kl_divergence(posterior_dist, prior_dist).sum()\n",
    "        \n",
    "        return neg_log_likelihood, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, data, optimizer, batch_size, val_data):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    train_loss_nll = 0\n",
    "    train_loss_kl = 0\n",
    "    T, n_train, _ = data.shape\n",
    "    _, n_val, _ = val_data.shape\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    idxs = np.random.permutation(n_train)\n",
    "    for i in range(0, n_train, batch_size):\n",
    "        nll, kl = model(data)\n",
    "        # want to maximize ll and minimize kl\n",
    "        loss = kl + nll\n",
    "        loss.backward()\n",
    "        \n",
    "        train_loss_nll += nll.item()\n",
    "        train_loss_kl += kl.item()\n",
    "        \n",
    "    optimizer.step()\n",
    "    end = time.time()\n",
    "    \n",
    "    val_loss = 0.\n",
    "    if val_data is not None:\n",
    "        n_val = len(val_data)\n",
    "        with torch.no_grad():\n",
    "            val_nll, val_kl = model(val_data)\n",
    "            loss = val_kl + val_nll\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(\"Epoch: {}, NLL Loss: {}, KL Div: {}, Val Loss: {}, Time took: {}\".format(epoch, train_loss_nll/(n_train*T), \\\n",
    "                                                                        train_loss_kl/(n_train*T), val_loss/n_val, \\\n",
    "                                                                        (end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model: nn.Module, data: torch.Tensor = None, val_data: torch.Tensor = None, lr = 1e-3, epoch=100, batch_size=64):\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch_number in range(epoch):\n",
    "        train(model, epoch_number, data, optimizer, batch_size, val_data)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dmm_data.load import loadSyntheticData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 10, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loadSyntheticData()\n",
    "torch.tensor(data['valid']['tensor']).to(device).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model = DMM().to(device)\n",
    "#     data = create_synthetic_data(num_sample=5000, time_step=25)\n",
    "    data = loadSyntheticData()\n",
    "    train_data = torch.tensor(data['train']['tensor']).transpose(0,1).float().to(device)\n",
    "#     val_data = create_synthetic_data(num_sample=150)\n",
    "    val_data = torch.tensor(data['valid']['tensor']).transpose(0,1).float().to(device)\n",
    "    trainer(model, data=train_data, val_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, NLL Loss: 6531.77752, KL Div: 482.53597125, Val Loss: 45087.096875, Time took: 4.147772312164307\n",
      "Epoch: 1, NLL Loss: 6502.3032575, KL Div: 473.157955, Val Loss: 44730.43125, Time took: 4.257425308227539\n",
      "Epoch: 2, NLL Loss: 6468.971035, KL Div: 464.02153375, Val Loss: 44401.15, Time took: 4.1038818359375\n",
      "Epoch: 3, NLL Loss: 6440.5760875, KL Div: 455.2881903125, Val Loss: 44360.003125, Time took: 4.263967752456665\n",
      "Epoch: 4, NLL Loss: 6410.5647925, KL Div: 446.7399396875, Val Loss: 44056.04375, Time took: 4.250853061676025\n",
      "Epoch: 5, NLL Loss: 6382.286125, KL Div: 438.4461834375, Val Loss: 43901.9625, Time took: 4.143693923950195\n",
      "Epoch: 6, NLL Loss: 6352.29315, KL Div: 430.3370934375, Val Loss: 43619.346875, Time took: 4.125028371810913\n",
      "Epoch: 7, NLL Loss: 6324.6470325, KL Div: 422.500525, Val Loss: 43088.50625, Time took: 4.2200026512146\n",
      "Epoch: 8, NLL Loss: 6296.6050075, KL Div: 414.8400184375, Val Loss: 42938.353125, Time took: 4.402120113372803\n",
      "Epoch: 9, NLL Loss: 6268.45923, KL Div: 407.41781921875, Val Loss: 42768.7625, Time took: 4.250420570373535\n",
      "Epoch: 10, NLL Loss: 6241.5729625, KL Div: 400.13021328125, Val Loss: 42461.253125, Time took: 4.290674686431885\n",
      "Epoch: 11, NLL Loss: 6215.53162, KL Div: 393.06073296875, Val Loss: 42268.503125, Time took: 4.253978967666626\n",
      "Epoch: 12, NLL Loss: 6187.394345, KL Div: 386.1062690625, Val Loss: 41974.565625, Time took: 4.3326756954193115\n",
      "Epoch: 13, NLL Loss: 6161.183515, KL Div: 379.36256171875, Val Loss: 41868.325, Time took: 4.194768905639648\n",
      "Epoch: 14, NLL Loss: 6135.8024825, KL Div: 372.8055178125, Val Loss: 41834.51875, Time took: 4.276393175125122\n",
      "Epoch: 15, NLL Loss: 6109.7872675, KL Div: 366.33330765625, Val Loss: 41705.63125, Time took: 4.207748889923096\n",
      "Epoch: 16, NLL Loss: 6084.8331075, KL Div: 360.14336359375, Val Loss: 41579.190625, Time took: 4.158112525939941\n",
      "Epoch: 17, NLL Loss: 6060.50373, KL Div: 354.0172065625, Val Loss: 41207.33125, Time took: 4.191452503204346\n",
      "Epoch: 18, NLL Loss: 6035.3352475, KL Div: 348.01227234375, Val Loss: 40951.65625, Time took: 4.24284291267395\n",
      "Epoch: 19, NLL Loss: 6010.08666, KL Div: 342.12227171875, Val Loss: 40683.090625, Time took: 4.317645311355591\n",
      "Epoch: 20, NLL Loss: 5984.6103525, KL Div: 336.3255359375, Val Loss: 40569.440625, Time took: 4.139089345932007\n",
      "Epoch: 21, NLL Loss: 5960.9736825, KL Div: 330.7488978125, Val Loss: 40327.321875, Time took: 4.1614460945129395\n",
      "Epoch: 22, NLL Loss: 5937.065045, KL Div: 325.2193221875, Val Loss: 40354.13125, Time took: 4.218422889709473\n",
      "Epoch: 23, NLL Loss: 5911.8277075, KL Div: 319.9090384375, Val Loss: 40113.225, Time took: 4.296856880187988\n",
      "Epoch: 24, NLL Loss: 5887.6048725, KL Div: 314.60179375, Val Loss: 39791.703125, Time took: 4.197446346282959\n",
      "Epoch: 25, NLL Loss: 5863.464575, KL Div: 309.40383203125, Val Loss: 39676.8625, Time took: 4.0900163650512695\n",
      "Epoch: 26, NLL Loss: 5838.6371625, KL Div: 304.30914171875, Val Loss: 39487.5625, Time took: 4.177204132080078\n",
      "Epoch: 27, NLL Loss: 5814.0394875, KL Div: 299.33812140625, Val Loss: 38942.334375, Time took: 4.263573408126831\n",
      "Epoch: 28, NLL Loss: 5790.13407, KL Div: 294.4612815625, Val Loss: 39083.540625, Time took: 4.146026611328125\n",
      "Epoch: 29, NLL Loss: 5765.79849, KL Div: 289.65909984375, Val Loss: 38745.103125, Time took: 4.166731595993042\n",
      "Epoch: 30, NLL Loss: 5740.5952375, KL Div: 285.02496375, Val Loss: 38604.70625, Time took: 4.35008692741394\n",
      "Epoch: 31, NLL Loss: 5715.21063, KL Div: 280.3630871875, Val Loss: 38683.946875, Time took: 4.401206970214844\n",
      "Epoch: 32, NLL Loss: 5689.8757375, KL Div: 275.82280609375, Val Loss: 38112.265625, Time took: 4.124954462051392\n",
      "Epoch: 33, NLL Loss: 5664.4075, KL Div: 271.3786121875, Val Loss: 38156.11875, Time took: 4.206636428833008\n",
      "Epoch: 34, NLL Loss: 5638.270105, KL Div: 267.03548328125, Val Loss: 37838.7, Time took: 4.175482273101807\n",
      "Epoch: 35, NLL Loss: 5612.3112575, KL Div: 262.71634359375, Val Loss: 37845.446875, Time took: 4.163142442703247\n",
      "Epoch: 36, NLL Loss: 5586.296085, KL Div: 258.47907078125, Val Loss: 37740.115625, Time took: 4.213665008544922\n",
      "Epoch: 37, NLL Loss: 5558.9290075, KL Div: 254.3261784375, Val Loss: 37349.21875, Time took: 4.20608925819397\n",
      "Epoch: 38, NLL Loss: 5531.9469675, KL Div: 250.2872859375, Val Loss: 37179.3, Time took: 4.301531076431274\n",
      "Epoch: 39, NLL Loss: 5504.9554125, KL Div: 246.30469296875, Val Loss: 36786.075, Time took: 4.2460503578186035\n",
      "Epoch: 40, NLL Loss: 5476.576615, KL Div: 242.36151296875, Val Loss: 36946.18125, Time took: 4.2790367603302\n",
      "Epoch: 41, NLL Loss: 5448.425955, KL Div: 238.43556015625, Val Loss: 36515.85625, Time took: 4.352192640304565\n",
      "Epoch: 42, NLL Loss: 5419.2972325, KL Div: 234.69189078125, Val Loss: 36348.703125, Time took: 4.264336824417114\n",
      "Epoch: 43, NLL Loss: 5389.24363, KL Div: 230.9464175, Val Loss: 35918.1875, Time took: 4.202115535736084\n",
      "Epoch: 44, NLL Loss: 5358.5144825, KL Div: 227.2637521875, Val Loss: 35928.95625, Time took: 4.214946985244751\n",
      "Epoch: 45, NLL Loss: 5327.8385425, KL Div: 223.6526821875, Val Loss: 35675.834375, Time took: 4.366961479187012\n",
      "Epoch: 46, NLL Loss: 5296.3043425, KL Div: 220.14328765625, Val Loss: 35177.796875, Time took: 4.290800094604492\n",
      "Epoch: 47, NLL Loss: 5264.0358625, KL Div: 216.70292859375, Val Loss: 35090.946875, Time took: 4.266881942749023\n",
      "Epoch: 48, NLL Loss: 5229.9504625, KL Div: 213.246053125, Val Loss: 34700.53125, Time took: 4.3568994998931885\n",
      "Epoch: 49, NLL Loss: 5195.3133575, KL Div: 209.97037375, Val Loss: 34488.6125, Time took: 4.258167505264282\n",
      "Epoch: 50, NLL Loss: 5161.6977875, KL Div: 206.682153125, Val Loss: 34431.846875, Time took: 4.305620193481445\n",
      "Epoch: 51, NLL Loss: 5125.687275, KL Div: 203.476580859375, Val Loss: 34199.990625, Time took: 4.261692523956299\n",
      "Epoch: 52, NLL Loss: 5089.4032025, KL Div: 200.372986796875, Val Loss: 34009.609375, Time took: 4.245146036148071\n",
      "Epoch: 53, NLL Loss: 5051.46462, KL Div: 197.34616078125, Val Loss: 33411.040625, Time took: 4.209391117095947\n",
      "Epoch: 54, NLL Loss: 5012.2724675, KL Div: 194.358740234375, Val Loss: 33443.175, Time took: 4.465732574462891\n",
      "Epoch: 55, NLL Loss: 4972.7513325, KL Div: 191.502254609375, Val Loss: 32946.634375, Time took: 4.274225234985352\n",
      "Epoch: 56, NLL Loss: 4931.474585, KL Div: 188.73204421875, Val Loss: 32908.6625, Time took: 4.228722810745239\n",
      "Epoch: 57, NLL Loss: 4889.05694, KL Div: 186.0247925, Val Loss: 32500.1875, Time took: 4.273286581039429\n",
      "Epoch: 58, NLL Loss: 4845.5991875, KL Div: 183.4339040625, Val Loss: 32120.1375, Time took: 4.262174367904663\n",
      "Epoch: 59, NLL Loss: 4801.362055, KL Div: 180.91795453125, Val Loss: 31728.984375, Time took: 4.0959718227386475\n",
      "Epoch: 60, NLL Loss: 4754.52011, KL Div: 178.526420078125, Val Loss: 31608.4125, Time took: 4.2757933139801025\n",
      "Epoch: 61, NLL Loss: 4706.882785, KL Div: 176.237538359375, Val Loss: 31165.065625, Time took: 4.073446035385132\n",
      "Epoch: 62, NLL Loss: 4658.00582, KL Div: 174.051901328125, Val Loss: 30847.909375, Time took: 4.225902318954468\n",
      "Epoch: 63, NLL Loss: 4607.743115, KL Div: 172.019870859375, Val Loss: 30425.7375, Time took: 4.185337781906128\n",
      "Epoch: 64, NLL Loss: 4556.1156175, KL Div: 170.082230390625, Val Loss: 30057.2375, Time took: 4.239892244338989\n",
      "Epoch: 65, NLL Loss: 4503.4787425, KL Div: 168.2901128125, Val Loss: 29755.05625, Time took: 4.315155744552612\n",
      "Epoch: 66, NLL Loss: 4448.8039925, KL Div: 166.6396278125, Val Loss: 29321.559375, Time took: 4.141098976135254\n",
      "Epoch: 67, NLL Loss: 4393.6757725, KL Div: 165.10236921875, Val Loss: 29019.459375, Time took: 4.316012620925903\n",
      "Epoch: 68, NLL Loss: 4336.5643275, KL Div: 163.726875, Val Loss: 28579.653125, Time took: 4.343492269515991\n",
      "Epoch: 69, NLL Loss: 4278.1616675, KL Div: 162.488691640625, Val Loss: 28207.98125, Time took: 4.144423484802246\n",
      "Epoch: 70, NLL Loss: 4219.135675, KL Div: 161.327056015625, Val Loss: 27707.528125, Time took: 4.153603553771973\n",
      "Epoch: 71, NLL Loss: 4158.89841, KL Div: 160.3282778125, Val Loss: 27396.821875, Time took: 4.096552610397339\n",
      "Epoch: 72, NLL Loss: 4098.64026, KL Div: 159.4491275, Val Loss: 26989.659375, Time took: 4.281116485595703\n",
      "Epoch: 73, NLL Loss: 4036.23561, KL Div: 158.6456978125, Val Loss: 26574.60625, Time took: 4.335458040237427\n",
      "Epoch: 74, NLL Loss: 3974.39119, KL Div: 157.931363359375, Val Loss: 26111.1125, Time took: 4.293647050857544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75, NLL Loss: 3911.964495, KL Div: 157.282063515625, Val Loss: 25934.334375, Time took: 4.065443515777588\n",
      "Epoch: 76, NLL Loss: 3850.25502, KL Div: 156.675699375, Val Loss: 25259.784375, Time took: 4.058127164840698\n",
      "Epoch: 77, NLL Loss: 3787.831135, KL Div: 156.05655234375, Val Loss: 24957.309375, Time took: 4.201474905014038\n",
      "Epoch: 78, NLL Loss: 3726.237705, KL Div: 155.400989765625, Val Loss: 24559.0046875, Time took: 3.957115411758423\n",
      "Epoch: 79, NLL Loss: 3665.3924375, KL Div: 154.688304140625, Val Loss: 24247.1671875, Time took: 4.100284099578857\n",
      "Epoch: 80, NLL Loss: 3605.688645, KL Div: 153.870247734375, Val Loss: 23763.8265625, Time took: 4.148946523666382\n",
      "Epoch: 81, NLL Loss: 3546.691155, KL Div: 152.952621953125, Val Loss: 23490.5890625, Time took: 4.242663145065308\n",
      "Epoch: 82, NLL Loss: 3489.479225, KL Div: 151.910897890625, Val Loss: 22979.2203125, Time took: 4.147438287734985\n",
      "Epoch: 83, NLL Loss: 3433.3776875, KL Div: 150.70961203125, Val Loss: 22756.240625, Time took: 4.390080451965332\n",
      "Epoch: 84, NLL Loss: 3379.4512325, KL Div: 149.39755875, Val Loss: 22242.8375, Time took: 4.265567064285278\n",
      "Epoch: 85, NLL Loss: 3327.2653925, KL Div: 147.947833203125, Val Loss: 21982.08125, Time took: 4.1304872035980225\n",
      "Epoch: 86, NLL Loss: 3276.71684625, KL Div: 146.41493109375, Val Loss: 21728.4296875, Time took: 4.214688062667847\n",
      "Epoch: 87, NLL Loss: 3227.93044125, KL Div: 144.81779921875, Val Loss: 21312.7375, Time took: 4.418513774871826\n",
      "Epoch: 88, NLL Loss: 3181.46017, KL Div: 143.15036484375, Val Loss: 21104.128125, Time took: 4.15846586227417\n",
      "Epoch: 89, NLL Loss: 3136.7070125, KL Div: 141.465916875, Val Loss: 20802.6265625, Time took: 4.181853294372559\n",
      "Epoch: 90, NLL Loss: 3094.11843625, KL Div: 139.7172584375, Val Loss: 20562.396875, Time took: 4.1619837284088135\n",
      "Epoch: 91, NLL Loss: 3053.1724675, KL Div: 137.9121165625, Val Loss: 20337.1796875, Time took: 4.1742942333221436\n",
      "Epoch: 92, NLL Loss: 3014.43592875, KL Div: 136.01642765625, Val Loss: 20006.8515625, Time took: 4.287637233734131\n",
      "Epoch: 93, NLL Loss: 2977.02789375, KL Div: 134.04376734375, Val Loss: 19746.2125, Time took: 4.308445453643799\n",
      "Epoch: 94, NLL Loss: 2941.58717375, KL Div: 132.040683828125, Val Loss: 19585.971875, Time took: 4.173490047454834\n",
      "Epoch: 95, NLL Loss: 2907.51239375, KL Div: 130.010102734375, Val Loss: 19371.3421875, Time took: 4.10123085975647\n",
      "Epoch: 96, NLL Loss: 2875.48103875, KL Div: 128.004633125, Val Loss: 19191.9515625, Time took: 4.271256446838379\n",
      "Epoch: 97, NLL Loss: 2844.37588625, KL Div: 126.035702265625, Val Loss: 18967.51875, Time took: 4.37311315536499\n",
      "Epoch: 98, NLL Loss: 2815.06585875, KL Div: 124.1626278125, Val Loss: 18644.8875, Time took: 4.155113935470581\n",
      "Epoch: 99, NLL Loss: 2786.96024625, KL Div: 122.40464859375, Val Loss: 18551.5453125, Time took: 4.212547302246094\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
