{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import kl_divergence\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_data(time_step = 10, num_sample = 16, marker_dim=3):\n",
    "    marker = torch.randn(time_step, num_sample, marker_dim).to(device)\n",
    "#     points_ = np.random.rand(time_step, num_sample) * 1.\n",
    "#     cum_sum_points =  np.cumsum(points_, axis = 0)\n",
    "#     t = np.stack([cum_sum_points, points_], axis = 2)\n",
    "#     x, t  = marker.tolist(), t.tolist()\n",
    "#     x = torch.tensor(x)\n",
    "#     t = torch.tensor(t)\n",
    "#     data = {'x':x, 't': t}\n",
    "    data = marker\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMM(nn.Module):\n",
    "    def __init__(self, marker_type='real', marker_dim=3, latent_dim=20):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus\n",
    "        \n",
    "        self.marker_type = marker_type\n",
    "        self.latent_dim = latent_dim\n",
    "        self.inference_hidden_dim = latent_dim\n",
    "        \n",
    "        self._inference_net(x_dim=marker_dim, z_dim=latent_dim, inference_hidden_dim=latent_dim)\n",
    "        self._gated_transition_net(latent_dim, latent_dim)\n",
    "        self._emission_net(z_dim=latent_dim, x_dim=marker_dim, emission_hidden_dim=latent_dim)\n",
    "\n",
    "    def _gated_transition_net(self, z_dim, transition_dim):\n",
    "        # mean\n",
    "        self.gating_unit_hidden = nn.Sequential(\n",
    "            nn.Linear(z_dim, transition_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.gating_unit_for_proposed_mean = nn.Sequential(\n",
    "            nn.Linear(transition_dim, z_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.nonlinear_proposed_mean = nn.Linear(transition_dim, z_dim)\n",
    "        \n",
    "        self.linear_mean = nn.Linear(z_dim, z_dim)\n",
    "        \n",
    "        self.linear_mean.weight.data = torch.eye(z_dim)\n",
    "        self.linear_mean.bias.data = torch.zeros(z_dim)\n",
    "        \n",
    "        # variance\n",
    "        self.z_next_var_net = nn.Sequential(\n",
    "            nn.Linear(z_dim, z_dim),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "    def _transition(self, z_prev):\n",
    "        \"\"\"\n",
    "            This is the function that computes the p_theta (z_t | z_{t-1})\n",
    "            Input:\n",
    "            z_prev: Tensor of shape TxBSxlatent_dim (if real)\n",
    "        \"\"\"\n",
    "        # Linear mean (linear function of previous state)\n",
    "        linear_proposed_mean = self.linear_mean(z_prev)\n",
    "        \n",
    "        # Nonlinear (proposed) mean using gates\n",
    "        hidden = self.gating_unit_hidden(z_prev)\n",
    "        gating_unit = self.gating_unit_for_proposed_mean(hidden)\n",
    "        proposed_mean = self.nonlinear_proposed_mean(hidden)\n",
    "        \n",
    "        # Combine nonlinear and linear options of means to choose\n",
    "        z_next_mean = (1 - gating_unit) * linear_proposed_mean \\\n",
    "                        + gating_unit*proposed_mean\n",
    "            \n",
    "        z_next_var_int = self.relu(proposed_mean)\n",
    "        z_next_var = self.z_next_var_net(z_next_var_int)\n",
    "        \n",
    "#         z_next_dist = Normal(z_next_mean, z_next_var.sqrt())\n",
    "#         z_next = z_next_dist.sample()\n",
    "        return z_next_mean, z_next_var\n",
    "\n",
    "    \n",
    "    def _emission_net(self, z_dim, x_dim, emission_hidden_dim):\n",
    "        self.emission_net = nn.Sequential(\n",
    "            nn.Linear(z_dim, emission_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emission_hidden_dim, emission_hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if self.marker_type == 'real':\n",
    "            self.x_mean = nn.Linear(emission_hidden_dim, x_dim)\n",
    "            self.x_var = nn.Sequential(\n",
    "                nn.Linear(emission_hidden_dim, x_dim),\n",
    "                nn.Softplus()\n",
    "            )\n",
    "        elif self.marker_type == 'binary':\n",
    "            pass\n",
    "    \n",
    "    def _emission(self, z_seq):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "            z_t: Tensor of shape TxBSxlatent_dim (if real)\n",
    "        \"\"\"\n",
    "        if self.marker_type == 'real':\n",
    "            x_mean = self.x_mean(z_seq)\n",
    "            x_var = self.x_var(z_seq)\n",
    "            x_dist = Normal(x_mean, x_var.sqrt())    \n",
    "        elif self.marker_type == 'binary':\n",
    "            pass\n",
    "#         x_sample = x_dist.sample()\n",
    "        return x_dist\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _combiner_fn(self, z_prev, rnn_state_right):\n",
    "        hidden_combined = (self.combiner_net(z_prev) + rnn_state_right)/3\n",
    "        \n",
    "        return hidden_combined\n",
    "    \n",
    "    def _inference_net(self, x_dim, z_dim, inference_hidden_dim):\n",
    "        \n",
    "        self.backward_rnn = nn.GRUCell(x_dim, inference_hidden_dim)\n",
    "        self.combiner_net = nn.Sequential(\n",
    "            nn.Linear(z_dim, inference_hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.posterior_mean_net = nn.Linear(inference_hidden_dim, z_dim)\n",
    "        self.posterior_var_net = nn.Sequential(\n",
    "            nn.Linear(inference_hidden_dim, z_dim),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "    def _inference(self, x_seq):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "            x_seq: Tensor of shape TxBSxmarker_dim (if real)\n",
    "        \"\"\"\n",
    "        # Generate z_0. Can choose to learn it as a parameter or fix it\n",
    "        # Recurrently, generate h_t\n",
    "        # Recurrently, generate z_t using z_{t-1} and h_t\n",
    "        T, BS, _ = x_seq.shape\n",
    "        z_dim = self.latent_dim\n",
    "        z_0 = nn.Parameter(torch.zeros(BS, z_dim)).to(device)\n",
    "        h_t = torch.zeros(BS, self.inference_hidden_dim).to(device)\n",
    "        z_seq = [z_0]\n",
    "        z_means = []\n",
    "        z_vars = []\n",
    "        \n",
    "        z_prev = z_0\n",
    "        # x_t[t] will be BS x marker_dim\n",
    "        # go backward, starting from T --> 1\n",
    "        for t in range(T-1,-1,-1):\n",
    "            h_t_right = self.backward_rnn(x_seq[t], h_t)\n",
    "            h_combined = self._combiner_fn(z_prev, h_t_right)\n",
    "            posterior_z_t_mean = self.posterior_mean_net(h_combined)\n",
    "            posterior_z_t_var = self.posterior_var_net(h_combined)\n",
    "            epsilon = torch.randn_like(posterior_z_t_mean)\n",
    "            posterior_z_t_sample = posterior_z_t_mean + posterior_z_t_var.sqrt()*epsilon\n",
    "\n",
    "            z_means.append(posterior_z_t_mean)\n",
    "            z_vars.append(posterior_z_t_var)\n",
    "            z_seq.append(posterior_z_t_sample)\n",
    "            z_prev = posterior_z_t_sample\n",
    "        \n",
    "        z_seq = torch.stack(z_seq, dim=0)\n",
    "        z_means = torch.stack(z_means, dim=0)\n",
    "        z_vars = torch.stack(z_vars, dim=0)\n",
    "        return z_seq, z_means, z_vars\n",
    "    \n",
    "    def forward(self, x_seq):\n",
    "        # Get a sampled sequence (z_0 --> z_T) and\n",
    "        # the means and the vars for\n",
    "        # q_phi(z_seq | x_seq) = q_phi(z_t | z_{t-1}, x_seq)\n",
    "        posterior_seq_sample, posterior_means, posterior_vars = self._inference(x_seq)\n",
    "        \n",
    "        # Get means and vars for p_theta (z) = p_theta (z_t | z_{t-1})\n",
    "        prior_means, prior_vars = self._transition(posterior_seq_sample[:-1])\n",
    "        \n",
    "        # Get p_theta (z_t | z_{t-1})\n",
    "        prior_dist = Normal(prior_means, prior_vars.sqrt())\n",
    "        \n",
    "        # Convert means and vars to torch distributions\n",
    "        posterior_dist = Normal(posterior_means, posterior_vars.sqrt())\n",
    "        \n",
    "        # Get p_theta(x_t | z_t)\n",
    "        reconstructed_x_dist = self._emission(posterior_seq_sample[:-1])\n",
    "        \n",
    "        # log p_theta (x_seq | z) = sum( log p_theta (x_t | z_t))\n",
    "        log_likelihood = -reconstructed_x_dist.log_prob(x_seq).sum()\n",
    "        \n",
    "        # KL divergence between the variational posterior distribution and generative prior\n",
    "        # KL (q_phi (z_t | z_{t-1}, x_seq) || p_theta (z_t | z_{t-1}))\n",
    "        kl = kl_divergence(posterior_dist, prior_dist).sum()\n",
    "        \n",
    "        return log_likelihood, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, data, optimizer, batch_size, val_data):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    n_train, n_val = len(data[0]), 1.\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    idxs = np.random.permutation(n_train)\n",
    "    for i in range(0, n_train, batch_size):\n",
    "        ll, kl = model(data)\n",
    "        # want to maximize ll - kl, therefore minimize the negative of that\n",
    "        loss = kl - ll\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "    optimizer.step()\n",
    "    end = time.time()\n",
    "    \n",
    "    val_loss = 0.\n",
    "    if val_data is not None:\n",
    "        n_val = len(val_data)\n",
    "        with torch.no_grad():\n",
    "            val_ll, val_kl = model(val_data)\n",
    "            loss = val_kl - val_ll\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(\"Epoch: {}, NLL Loss: {}, Val Loss: {}, Time took: {}\".format(epoch, train_loss/n_train,\\\n",
    "                                                                        val_loss/n_val, (end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model: nn.Module, data: torch.Tensor = None, val_data: torch.Tensor = None, lr = 1e-3, epoch=100, batch_size=64):\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch_number in range(epoch):\n",
    "        train(model, epoch_number, data, optimizer, batch_size, val_data)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model = DMM().to(device)\n",
    "    data = create_synthetic_data(num_sample=5000, time_step=25)\n",
    "    val_data = create_synthetic_data(num_sample=150)\n",
    "    trainer(model, data=data, val_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, NLL Loss: -306.35235546875, Val Loss: -321.7994140625, Time took: 0.2870516777038574\n",
      "Epoch: 1, NLL Loss: -318.53726171875, Val Loss: -306.61875, Time took: 0.25124239921569824\n",
      "Epoch: 2, NLL Loss: -332.0558828125, Val Loss: -325.8939453125, Time took: 0.2702515125274658\n",
      "Epoch: 3, NLL Loss: -343.149201171875, Val Loss: -334.184521484375, Time took: 0.35918617248535156\n",
      "Epoch: 4, NLL Loss: -358.420451171875, Val Loss: -353.5908203125, Time took: 0.420757532119751\n",
      "Epoch: 5, NLL Loss: -369.276701171875, Val Loss: -357.4369140625, Time took: 0.2658872604370117\n",
      "Epoch: 6, NLL Loss: -380.89758984375, Val Loss: -355.3448486328125, Time took: 0.2644665241241455\n",
      "Epoch: 7, NLL Loss: -391.230166015625, Val Loss: -386.770947265625, Time took: 0.24984431266784668\n",
      "Epoch: 8, NLL Loss: -402.086724609375, Val Loss: -395.80068359375, Time took: 0.3705635070800781\n",
      "Epoch: 9, NLL Loss: -413.643384765625, Val Loss: -395.46435546875, Time took: 0.24553585052490234\n",
      "Epoch: 10, NLL Loss: -424.116041015625, Val Loss: -408.066748046875, Time took: 0.27253103256225586\n",
      "Epoch: 11, NLL Loss: -434.226015625, Val Loss: -425.401123046875, Time took: 0.2802610397338867\n",
      "Epoch: 12, NLL Loss: -445.681818359375, Val Loss: -431.807568359375, Time took: 0.28128886222839355\n",
      "Epoch: 13, NLL Loss: -456.156181640625, Val Loss: -454.443603515625, Time took: 0.2783663272857666\n",
      "Epoch: 14, NLL Loss: -466.565865234375, Val Loss: -452.36162109375, Time took: 0.28127169609069824\n",
      "Epoch: 15, NLL Loss: -475.005021484375, Val Loss: -452.201611328125, Time took: 0.38848066329956055\n",
      "Epoch: 16, NLL Loss: -484.451046875, Val Loss: -469.0517578125, Time took: 0.29609179496765137\n",
      "Epoch: 17, NLL Loss: -494.80174609375, Val Loss: -474.2673828125, Time took: 0.2889595031738281\n",
      "Epoch: 18, NLL Loss: -504.219646484375, Val Loss: -496.30185546875, Time took: 0.3960232734680176\n",
      "Epoch: 19, NLL Loss: -513.562599609375, Val Loss: -497.7564453125, Time took: 0.3023691177368164\n",
      "Epoch: 20, NLL Loss: -523.57804296875, Val Loss: -500.647119140625, Time took: 0.28014111518859863\n",
      "Epoch: 21, NLL Loss: -533.38080078125, Val Loss: -527.42333984375, Time took: 0.27895140647888184\n",
      "Epoch: 22, NLL Loss: -540.711859375, Val Loss: -514.71123046875, Time took: 0.28127360343933105\n",
      "Epoch: 23, NLL Loss: -550.35211328125, Val Loss: -543.26396484375, Time took: 0.27948451042175293\n",
      "Epoch: 24, NLL Loss: -560.101375, Val Loss: -550.10908203125, Time took: 0.2806835174560547\n",
      "Epoch: 25, NLL Loss: -566.2578515625, Val Loss: -550.1876953125, Time took: 0.3211383819580078\n",
      "Epoch: 26, NLL Loss: -578.93440625, Val Loss: -557.20068359375, Time took: 0.34280896186828613\n",
      "Epoch: 27, NLL Loss: -586.75244140625, Val Loss: -546.965625, Time took: 0.2966172695159912\n",
      "Epoch: 28, NLL Loss: -594.35880859375, Val Loss: -572.96533203125, Time took: 0.2898266315460205\n",
      "Epoch: 29, NLL Loss: -604.913, Val Loss: -592.428125, Time took: 0.3825082778930664\n",
      "Epoch: 30, NLL Loss: -612.47142578125, Val Loss: -595.07705078125, Time took: 0.3027462959289551\n",
      "Epoch: 31, NLL Loss: -622.79016015625, Val Loss: -601.5111328125, Time took: 0.28642868995666504\n",
      "Epoch: 32, NLL Loss: -632.884171875, Val Loss: -618.322607421875, Time took: 0.2809436321258545\n",
      "Epoch: 33, NLL Loss: -641.3820703125, Val Loss: -631.31962890625, Time took: 0.28543519973754883\n",
      "Epoch: 34, NLL Loss: -647.85286328125, Val Loss: -653.73828125, Time took: 0.2807188034057617\n",
      "Epoch: 35, NLL Loss: -660.03221875, Val Loss: -622.3236328125, Time took: 0.27910661697387695\n",
      "Epoch: 36, NLL Loss: -668.8648984375, Val Loss: -660.92412109375, Time took: 0.33820033073425293\n",
      "Epoch: 37, NLL Loss: -678.69099609375, Val Loss: -655.566259765625, Time took: 0.34017515182495117\n",
      "Epoch: 38, NLL Loss: -686.14331640625, Val Loss: -669.33916015625, Time took: 0.26885342597961426\n",
      "Epoch: 39, NLL Loss: -698.839984375, Val Loss: -654.722802734375, Time took: 0.2205181121826172\n",
      "Epoch: 40, NLL Loss: -709.35459375, Val Loss: -659.029443359375, Time took: 0.36868953704833984\n",
      "Epoch: 41, NLL Loss: -719.2801328125, Val Loss: -679.37431640625, Time took: 0.28929829597473145\n",
      "Epoch: 42, NLL Loss: -729.26478515625, Val Loss: -704.421484375, Time took: 0.2880721092224121\n",
      "Epoch: 43, NLL Loss: -740.2477890625, Val Loss: -709.81865234375, Time took: 0.27739596366882324\n",
      "Epoch: 44, NLL Loss: -752.78236328125, Val Loss: -700.61005859375, Time took: 0.2739756107330322\n",
      "Epoch: 45, NLL Loss: -763.5439453125, Val Loss: -725.75966796875, Time took: 0.27500224113464355\n",
      "Epoch: 46, NLL Loss: -774.30434765625, Val Loss: -733.282275390625, Time took: 0.2734553813934326\n",
      "Epoch: 47, NLL Loss: -786.97666015625, Val Loss: -747.95810546875, Time took: 0.32898759841918945\n",
      "Epoch: 48, NLL Loss: -800.4452265625, Val Loss: -781.34599609375, Time took: 0.40915989875793457\n",
      "Epoch: 49, NLL Loss: -812.70824609375, Val Loss: -809.642138671875, Time took: 0.3519411087036133\n",
      "Epoch: 50, NLL Loss: -826.44671875, Val Loss: -825.1787109375, Time took: 0.294605016708374\n",
      "Epoch: 51, NLL Loss: -838.27625390625, Val Loss: -790.85107421875, Time took: 0.40705442428588867\n",
      "Epoch: 52, NLL Loss: -855.8324296875, Val Loss: -800.9482421875, Time took: 0.27166247367858887\n",
      "Epoch: 53, NLL Loss: -871.90862890625, Val Loss: -864.56044921875, Time took: 0.2722170352935791\n",
      "Epoch: 54, NLL Loss: -888.8039921875, Val Loss: -883.12060546875, Time took: 0.2452092170715332\n",
      "Epoch: 55, NLL Loss: -902.26355078125, Val Loss: -882.026171875, Time took: 0.242600679397583\n",
      "Epoch: 56, NLL Loss: -921.71983984375, Val Loss: -902.62099609375, Time took: 0.26388049125671387\n",
      "Epoch: 57, NLL Loss: -941.16397265625, Val Loss: -888.40302734375, Time took: 0.21708250045776367\n",
      "Epoch: 58, NLL Loss: -961.08562109375, Val Loss: -940.33427734375, Time took: 0.26985883712768555\n",
      "Epoch: 59, NLL Loss: -984.59451171875, Val Loss: -952.87958984375, Time took: 0.40359020233154297\n",
      "Epoch: 60, NLL Loss: -1006.60396875, Val Loss: -980.73046875, Time took: 0.34872937202453613\n",
      "Epoch: 61, NLL Loss: -1034.2418984375, Val Loss: -1010.1572265625, Time took: 0.3739957809448242\n",
      "Epoch: 62, NLL Loss: -1060.72230859375, Val Loss: -1080.9498046875, Time took: 0.2758028507232666\n",
      "Epoch: 63, NLL Loss: -1093.56515625, Val Loss: -1080.1294921875, Time took: 0.27575254440307617\n",
      "Epoch: 64, NLL Loss: -1116.846921875, Val Loss: -1105.96279296875, Time took: 0.27571582794189453\n",
      "Epoch: 65, NLL Loss: -1165.1435, Val Loss: -1165.9341796875, Time took: 0.275592565536499\n",
      "Epoch: 66, NLL Loss: -1199.9419375, Val Loss: -1190.95078125, Time took: 0.2758798599243164\n",
      "Epoch: 67, NLL Loss: -1238.21115625, Val Loss: -1254.44873046875, Time took: 0.27604246139526367\n",
      "Epoch: 68, NLL Loss: -1297.7335390625, Val Loss: -1298.67978515625, Time took: 0.33954668045043945\n",
      "Epoch: 69, NLL Loss: -1351.1274453125, Val Loss: -1374.573828125, Time took: 0.29781365394592285\n",
      "Epoch: 70, NLL Loss: -1405.3060546875, Val Loss: -1340.96865234375, Time took: 0.2754790782928467\n",
      "Epoch: 71, NLL Loss: -1486.0996640625, Val Loss: -1495.07470703125, Time took: 0.30258750915527344\n",
      "Epoch: 72, NLL Loss: -1571.055234375, Val Loss: -1551.275, Time took: 0.3221926689147949\n",
      "Epoch: 73, NLL Loss: -1643.8381875, Val Loss: -1665.9482421875, Time took: 0.2752649784088135\n",
      "Epoch: 74, NLL Loss: -1765.9840078125, Val Loss: -1730.344921875, Time took: 0.27510857582092285\n",
      "Epoch: 75, NLL Loss: -1883.409109375, Val Loss: -1909.074609375, Time took: 0.2751486301422119\n",
      "Epoch: 76, NLL Loss: -2031.3505625, Val Loss: -2010.6509765625, Time took: 0.2770040035247803\n",
      "Epoch: 77, NLL Loss: -2203.91753125, Val Loss: -2248.84609375, Time took: 0.27556705474853516\n",
      "Epoch: 78, NLL Loss: -2402.25328125, Val Loss: -2273.5884765625, Time took: 0.3083760738372803\n",
      "Epoch: 79, NLL Loss: -2634.673125, Val Loss: -2860.833203125, Time took: 0.32662296295166016\n",
      "Epoch: 80, NLL Loss: -2909.639828125, Val Loss: -3021.494140625, Time took: 0.28452515602111816\n",
      "Epoch: 81, NLL Loss: -3237.3363125, Val Loss: -3211.471875, Time took: 0.2691986560821533\n",
      "Epoch: 82, NLL Loss: -3669.441859375, Val Loss: -3814.783984375, Time took: 0.2129969596862793\n",
      "Epoch: 83, NLL Loss: -4165.32153125, Val Loss: -4292.62578125, Time took: 0.36424875259399414\n",
      "Epoch: 84, NLL Loss: -4779.6055, Val Loss: -4933.83359375, Time took: 0.24704217910766602\n",
      "Epoch: 85, NLL Loss: -5443.837625, Val Loss: -5480.816796875, Time took: 0.24724388122558594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86, NLL Loss: -6304.22571875, Val Loss: -6915.47734375, Time took: 0.24769234657287598\n",
      "Epoch: 87, NLL Loss: -7397.0903125, Val Loss: -7903.09375, Time took: 0.25017309188842773\n",
      "Epoch: 88, NLL Loss: -8739.37565625, Val Loss: -9640.79296875, Time took: 0.25121188163757324\n",
      "Epoch: 89, NLL Loss: -10242.9719375, Val Loss: -10105.78125, Time took: 0.2428126335144043\n",
      "Epoch: 90, NLL Loss: -12209.9978125, Val Loss: -12383.54921875, Time took: 0.2744133472442627\n",
      "Epoch: 91, NLL Loss: -14566.4756875, Val Loss: -18554.2265625, Time took: 0.34703564643859863\n",
      "Epoch: 92, NLL Loss: -17671.6365625, Val Loss: -20096.8484375, Time took: 0.2709007263183594\n",
      "Epoch: 93, NLL Loss: -21692.634375, Val Loss: -25919.8625, Time took: 0.27404356002807617\n",
      "Epoch: 94, NLL Loss: -25846.2455, Val Loss: -27557.325, Time took: 0.35396409034729004\n",
      "Epoch: 95, NLL Loss: -32020.518125, Val Loss: -34979.95625, Time took: 0.35999059677124023\n",
      "Epoch: 96, NLL Loss: -38054.328, Val Loss: -49078.228125, Time took: 0.3370182514190674\n",
      "Epoch: 97, NLL Loss: -47369.668, Val Loss: -53392.375, Time took: 0.3015596866607666\n",
      "Epoch: 98, NLL Loss: -58196.752, Val Loss: -76397.725, Time took: 0.2943851947784424\n",
      "Epoch: 99, NLL Loss: -71255.16, Val Loss: -84806.7625, Time took: 0.29499220848083496\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
